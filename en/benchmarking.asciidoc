[#benchmarking]
= Benchmarking

> Measure twice and cut once. -- Ancient proverb

Before we attempt to improve the performance of a piece of code, first we must know its current performance.

This section focuses on how to construct useful benchmarks using the Go testing framework, and gives practical tips for avoiding the pitfalls.

== Benchmarking ground rules

Before you benchmark, you must have a stable environment to get repeatable results.

- The machine must be idle--don't profile on shared hardware, don't browse the web while waiting for a long benchmark to run.
- Watch out for power saving and thermal scaling. These are almost unavoidable on modern laptops.
- Avoid virtual machines and shared cloud hosting; they can be too noisy for consistent measurements.

If you can afford it, buy dedicated performance test hardware.
Rack it, disable all the power management and thermal scaling, and never update the software on those machines.
The last point is poor advice from a system adminstration point of view, but if a software update changes the way the kernel or library performs--think the Spectre patches--this will invalidate any previous benchmarking results.

For the rest of us, have a before and after sample and run them multiple times to get consistent results.

== Using the testing package for benchmarking

The `testing` package has built in support for writing benchmarks.
If we have a simple function like this:
[source,go]
----
include::../examples/fib/fib_test.go[tags=fib]
----
The we can use the `testing` package to write a _benchmark_ for the function using this form.
[source,go]
----
include::../examples/fib/fib_test.go[tags=benchmarkfib20]
----
TIP: The benchmark function lives alongside your tests in a `_test.go` file.

Benchmarks are similar to tests, the only real difference is they take a `*testing.B` rather than a `*testing.T`.
Both of these types implement the `testing.TB` interface which provides crowd favorites like `Errorf()`, `Fatalf()`, and `FailNow()`.

=== Running a package's benchmarks

As benchmarks use the `testing` package they are executed via the `go test` subcommand.
However, by default when you invoke `go test`, benchmarks are excluded. 

To explicitly run benchmarks in a package use the `-bench` flag. `-bench` takes a regular expression that matches the names of the benchmarks you want to run, so the most common way to invoke all benchmarks in a package is `-bench=.`.
Here is an example:
[source,options=nowrap]
% go test -bench=. ./examples/fib/
goos: darwin
goarch: amd64
pkg: high-performance-go-workshop/examples/fib
BenchmarkFib20-8           28947             40617 ns/op
PASS
ok      high-performance-go-workshop/examples/fib       1.602s

[NOTE]
====
`go test` will also run all the tests in a package before matching benchmarks, so if you have a lot of tests in a package, or they take a long time to run, you can exclude them by providing `go test`'s `-run` flag with a regex that matches nothing; ie.
[source]
go test -run=^$ 
====

=== How benchmarks work

Each benchmark function is called with different value for `b.N`, this is the number of iterations the benchmark should run for.

`b.N` starts at 1, if the benchmark function completes in under 1 second--the default--then `b.N` is increased and the benchmark function run again.

`b.N` increases in the approximate sequence, growing by roughly 20% each iteration.
The benchmark framework tries to be smart and if it sees small values of `b.N` are completing relatively quickly, it will increase the iteration count faster.

Looking at the example above, `BenchmarkFib20-8` found that around 29,000 iterations of the loop took just over a second.
From there the benchmark framework computed that the average time per operation was 40617ns.

[NOTE]
====
The `-8` suffix relates to the value of `GOMAXPROCS` that was used to run this test.
This number, like `GOMAXPROCS`, defaults to the number of CPUs visible to the Go process on startup.
You can change this value with the `-cpu` flag which takes a list of values to run the benchmark with.
[source,options="nowrap"]
----
% go test -bench=. -cpu=1,2,4 ./examples/fib/
goos: darwin
goarch: amd64
pkg: high-performance-go-workshop/examples/fib
BenchmarkFib20             31479             37987 ns/op
BenchmarkFib20-2           31846             37859 ns/op
BenchmarkFib20-4           31716             39255 ns/op
PASS
ok      high-performance-go-workshop/examples/fib       4.805s
----
This shows running the benchmark with 1, 2, and 4 cores.
In this case the flag has little effect on the outcome because this benchmark is entirely sequential.
====

=== Go 1.13 benchmarking changes

Prior to Go 1.13 the benchmarking iteration numbers we rounded to a 1, 2, 3, 5 sequence.
The original goal of this rounding to was to make it easier to eyeball times.
However, proper analysis requires tooling anyway, so human readable numbers became less valuble as the tooling improved.

The rounding could hide almost an order of magnitude of variation.

Fortuntely in Go 1.13 the rounding has been removed, which improves the accuracy of benchmarking operations in the low ns/op range, and reduce the run time of benchmarks overall as the benchmark framework arrives at the correct iteration count faster.

=== Improving benchmark accuracy

The `fib` function is a slightly contrived example--unless your writing a TechPower web server benchmark--it's unlikely your business is going to be gated on how quickly you can compute the 20th number in the Fibonaci sequence.
But, the benchmark does provide a faithful example of a valid benchmark. 

Specifically you want your benchmark to run for several tens of thousand iterations so you get a good average per operation.
If your benchmark runs for only 100's or 10's of iterations, the average of those runs may have a high standard deviation.
If your benchmark runs for millions or billions of iterations, the average may be very accurate, but subject to the vaguaries of code layout and alignment.

To increase the number of iterations, the benchmark time can be increased with the `-benchtime` flag. For example:
[source,options=nowrap]
% go test -bench=. -benchtime=10s ./examples/fib/
goos: darwin
goarch: amd64
pkg: high-performance-go-workshop/examples/fib
BenchmarkFib20-8          313048             41673 ns/op
PASS
ok      high-performance-go-workshop/examples/fib       13.442s

Ran the same benchmark until it reached a value of `b.N` that took longer than 10 seconds to return.
As we're running for 10x longer, the total number of iterations is 10x larger.
The result hasn't changed much, which is what we expected.

====
Why is the total time reporteded to be 13 seconds, not 10?
====

If you have a benchmark which runs for millons or billions of iterations resulting in a time per operation in the micro or nano second range, you may find that your benchmark numbers are unstable because thermal scaling, memory locality, background processing, gc activity, etc.

For times measured in 10 or single digit nanoseconds per operation the relativistic effects of instruction reordering and code alignment will have an impact on your benchmark times.

To address this run benchmarks multiple times with the `-count` flag:
[source,options=nowrap]
% go test -bench=Fib20 -count=10 ./examples/fib/ | tee old.txt
goos: darwin
goarch: amd64
pkg: high-performance-go-workshop/examples/fib
BenchmarkFib20-8           30099             38117 ns/op
BenchmarkFib20-8           31806             40433 ns/op
BenchmarkFib20-8           30052             43412 ns/op
BenchmarkFib20-8           28392             39225 ns/op
BenchmarkFib20-8           28270             42956 ns/op
BenchmarkFib20-8           28276             49493 ns/op
BenchmarkFib20-8           26047             45571 ns/op
BenchmarkFib20-8           27392             43803 ns/op
BenchmarkFib20-8           27507             44896 ns/op
BenchmarkFib20-8           25647             43579 ns/op
PASS
ok      high-performance-go-workshop/examples/fib       16.516s

A benchmark of `Fib(1)` takes around 2 nano seconds with a variance of _+/- 15%_. 

New in Go 1.12 is the `-benchtime` flag now takes a number of iterations, eg. `-benchtime=20x` which will run your code exactly `benchtime` times. 

====
Try running the fib bench above with a `-benchtime` of 10x, 20x, 50x, 100x, and 300x. What do you see?
====

TIP: If you find that the defaults that `go test` applies need to be tweaked for a particular package, I suggest codifying those settings in a `Makefile` so everyone who wants to run your benchmarks can do so with the same settings.

== Comparing benchmarks with benchstat

In the previous section I suggested running benchmarks more than once to get more data to average.
This is good advice for any benchmark because of the effects of power management, background processes, and thermal management that I mentioned at the start of the chapter.

I'm going to introduce a tool by Russ Cox called https://godoc.org/golang.org/x/perf/cmd/benchstat[benchstat].

[source,options=nowrap]
% go get golang.org/x/perf/cmd/benchstat

Benchstat can take a set of benchmark runs and tell you how stable they are. Here is an example of `Fib(20)` on battery power.
[source,options=nowrap]
----
% go test -bench=Fib20 -count=10 ./examples/fib/ | tee old.txt
goos: darwin
goarch: amd64
pkg: high-performance-go-workshop/examples/fib
BenchmarkFib20-8           30721             37893 ns/op
BenchmarkFib20-8           31468             38695 ns/op
BenchmarkFib20-8           31726             37521 ns/op
BenchmarkFib20-8           31686             37583 ns/op
BenchmarkFib20-8           31719             38087 ns/op
BenchmarkFib20-8           31802             37703 ns/op
BenchmarkFib20-8           31754             37471 ns/op
BenchmarkFib20-8           31800             37570 ns/op
BenchmarkFib20-8           31824             37644 ns/op
BenchmarkFib20-8           31165             38354 ns/op
PASS
ok      high-performance-go-workshop/examples/fib       15.808s
% benchstat old.txt 
name     time/op
Fib20-8  37.9µs ± 2%
----

`benchstat` tells us the mean is 38.8 microseconds with a +/- 2% variation across the samples.
This is because while the benchmark was running I didn't touch the machine.

=== Improve `Fib`
Determining the performance delta between two sets of benchmarks can be tedious and error prone. Benchstat can help us with this. 

[TIP]
====
Saving the output from a benchmark run is useful, but you can also save the _binary_ that produced it. This lets you rerun benchmark previous iterations. To do this, use the `-c` flag to save the test binary--I often rename this binary from `.test` to `.golden`.
----
% go test -c
% mv fib.test fib.golden 
----
====

The previous `Fib` fuction had hard coded values for the 0th and 1st numbers in the fibonaci series. After that the code calls itself recursively.
We'll talk about the cost of recursion later today, but for the moment, assume it has a cost, especially as our algorithm uses exponential time.

As simple fix to this would be to hard code another number from the fibonacci series, reducing the depth of each recusive call by one.
[source,go,options=nowrap]
----
include::{docdir}/../examples/fib/fib_nobuild.go[tags=fib]
----

TIP: This file also includes a comprehensive test for `Fib`. Don't try to improve your benchmarks without a test that verifies the current behaviour.

To compare our new version, we compile a new test binary and benchmark both of them and use `benchstat` to compare the outputs.

[source,options=nowrap]
% go test -c
% ./fib.golden -test.bench=. -test.count=10 > old.txt
% ./fib.test -test.bench=. -test.count=10 > new.txt
% benchstat old.txt new.txt
name     old time/op  new time/op  delta
Fib20-8  37.9µs ± 2%  24.1µs ± 3%  -36.26%  (p=0.000 n=10+10)

There are three things to check when comparing benchmarks

- The variance ± in the old and new times. 1-2% is good, 3-5% is ok, greater than 5% and some of your samples will be considered unreliable. Be careful when comparing benchmarks where one side has a high variance, you may not be seeing an improvement.
- Missing samples. benchstat will report how many of the old and new samples it considered to be valid, sometimes you may find only, say, 9 reported, even though you did `-count=10`. A 10% or lower rejection rate is ok, higher than 10% may indicate your setup is unstable and you may be comparing too few samples.

=== Beware the p-value

p-values lower than 0.05 likely to be statistiaclly significant.
p-values greater than 0.05 imply the benchmark may not be statistically significant.

https://go-review.googlesource.com/c/go/+/171736[An example of questionable p-values].

Further reading: https://en.wikipedia.org/wiki/P-value[P-value (wikipedia)].

== Avoiding benchmarking start up costs

Sometimes your benchmark has a once per run setup cost. `b.ResetTimer()` will can be used to ignore the time accrued in setup.
[source,go]
----
func BenchmarkExpensive(b *testing.B) {
        boringAndExpensiveSetup()
        b.ResetTimer() // <1>
        for n := 0; n < b.N; n++ {
                // function under test
        }
}
----
<1> Reset the benchmark timer

If you have some expensive setup logic _per loop_ iteration, use `b.StopTimer()` and `b.StartTimer()` to pause the benchmark timer.
[source,go]
----
func BenchmarkComplicated(b *testing.B) {
        for n := 0; n < b.N; n++ {
                b.StopTimer() // <1>
                complicatedSetup()
                b.StartTimer() // <2>
                // function under test
        }
}
----
<1> Pause benchmark timer
<2> Resume timer

== Benchmarking allocations

Allocation count and size is strongly correlated with benchmark time.
You can tell the `testing` framework to record the number of allocations made by code under test.
[source,go]
----
func BenchmarkRead(b *testing.B) {
        b.ReportAllocs()
        for n := 0; n < b.N; n++ {
                // function under test
        }
}
----
Here is an example using the `bufio` package's benchmarks.
[source,options=nowrap]
----
% go test -run=^$ -bench=. bufio
goos: darwin
goarch: amd64
pkg: bufio
BenchmarkReaderCopyOptimal-8            12999212                78.6 ns/op
BenchmarkReaderCopyUnoptimal-8           8495018               133 ns/op
BenchmarkReaderCopyNoWriteTo-8            360471              2805 ns/op
BenchmarkReaderWriteToOptimal-8          3839959               291 ns/op
BenchmarkWriterCopyOptimal-8            13878241                82.7 ns/op
BenchmarkWriterCopyUnoptimal-8           9932562               117 ns/op
BenchmarkWriterCopyNoReadFrom-8           385789              2681 ns/op
BenchmarkReaderEmpty-8                   1863018               640 ns/op            4224 B/op          3 allocs/op
BenchmarkWriterEmpty-8                   2040326               579 ns/op            4096 B/op          1 allocs/op
BenchmarkWriterFlush-8                  88363759                12.7 ns/op             0 B/op          0 allocs/op
PASS
ok      bufio   13.249s
----

[TIP]
====
You can also use the `go test -benchmem` flag to force the testing framework to report allocation statistics for all benchmarks run.
[source,options=nowrap]
----
%  go test -run=^$ -bench=. -benchmem bufio
goos: darwin
goarch: amd64
pkg: bufio
BenchmarkReaderCopyOptimal-8            13860543                82.8 ns/op            16 B/op          1 allocs/op
BenchmarkReaderCopyUnoptimal-8           8511162               137 ns/op              32 B/op          2 allocs/op
BenchmarkReaderCopyNoWriteTo-8            379041              2850 ns/op           32800 B/op          3 allocs/op
BenchmarkReaderWriteToOptimal-8          4013404               280 ns/op              16 B/op          1 allocs/op
BenchmarkWriterCopyOptimal-8            14132904                82.7 ns/op            16 B/op          1 allocs/op
BenchmarkWriterCopyUnoptimal-8          10487898               113 ns/op              32 B/op          2 allocs/op
BenchmarkWriterCopyNoReadFrom-8           362676              2816 ns/op           32800 B/op          3 allocs/op
BenchmarkReaderEmpty-8                   1857391               639 ns/op            4224 B/op          3 allocs/op
BenchmarkWriterEmpty-8                   2041264               577 ns/op            4096 B/op          1 allocs/op
BenchmarkWriterFlush-8                  87643513                12.5 ns/op             0 B/op          0 allocs/op
PASS
ok      bufio   13.430s
----
====

== Watch out for compiler optimisations

This example comes from https://github.com/golang/go/issues/14813#issue-140603392[issue 14813].
[source,go,options=nowrap]
----
include::../examples/popcnt/popcnt_test.go[tags=popcnt]
----

How fast do you think this function will benchmark? Let's find out.
----
% go test -bench=. ./examples/popcnt/
goos: darwin
goarch: amd64
pkg: high-performance-go-workshop/examples/popcnt
BenchmarkPopcnt-8       1000000000               0.278 ns/op
PASS
ok      high-performance-go-workshop/examples/popcnt    0.318s
----
0.278 of a nano second; that's basically one clock cycle.
Even assuming that the CPU may have a few instructions in flight per clock tick, this number seems unreasonably low.
What happened?

To understand what happened, we have to look at the function under benchmake, `popcnt`.
`popcnt` is a leaf function -- it does not call any other functions -- so the compiler can inline it.

Because the function is inlined, the compiler now can see it has no side effects.
`popcnt` does not affect the state of any global variable.
Thus, the call is eliminated.
This is what the compiler sees:
[source,go,options=nowrap]
include::../examples/popcnt/popcnt_nobuild.go[tags=benchmark]

On all versions of the Go compiler that i've tested, the loop is still generated.
But Intel CPUs are really good at optimising loops, especially empty ones. 

=== Exercise, look at the assembly

Before we go on, lets look at the assembly to confirm what we saw

[source]
% go test -gcflags=-S

====
Use `gcflags="-l -S" to disable inlining, how does that affect the assembly output
====

[NOTE]
.Optimisation is a good thing
====
The thing to take away is the same optimisations that _make real code fast_, by removing unnecessary computation, are the same ones that remove benchmarks that have no observable side effects.

This is only going to get more common as the Go compiler improves.
====

=== Fixing the benchmark

Disabling inlining to make the benchmark work is unrealistic; we want to build our code with optimisations on.

To fix this benchmark we must ensure that the compiler cannot _prove_ that the body of `BenchmarkPopcnt` does not cause global state to change.
[source,go,options=nowrap]
----
include::../examples/popcnt/popcnt2_test.go[tags=benchmark]
----
This is the recommended way to ensure the compiler cannot optimise away body of the loop.

First we _use_ the result of calling `popcnt` by storing it in `r`.
Second, because `r` is declared locally inside the scope of `BenchmarkPopcnt` once the benchmark is over, the result of `r` is never visible to another part of the program, so as the final act we assign the value of `r` to the package public variable `Result`. 

Because `Result` is public the compiler cannot prove that another package importing this one will not be able to see the value of `Result` changing over time, hence it cannot optimise away any of the operations leading to its assignment.

====
What happens if we assign to `Result` directly? Does this affect the benchmark time? What about if we assign the result of `popcnt` to `_`?
====

WARNING: In our earlier `Fib` benchmark we didn't take these precautions, should we have done so?

== Benchmark mistakes

The `for` loop is crucial to the operation of the benchmark.

Here are two incorrect benchmarks, can you explain what is wrong with them?

[source]
----
include::../examples/benchfib/wrong_test.go[tags=wrong]
----

[source]
----
include::../examples/benchfib/wrong_test.go[tags=wrong2]
----

====
Run these benchmarks, what do you see?
====

== Benchmarking `math/rand`

Thanks to Spectre and Meltdown we all know that computers are very good a caching predictable operations.
Perhaps our Popcnt benchmark, even the correct version, is returning us a cached value--data that varies unpredictably might be slower than we're expecting. Let's test this.

[source]
----
include::../examples/popcnt-rand/popcnt_test.go[tags=benchmark]
----

====
Is this result reliable? If not, what went wrong?
====

== Profiling benchmarks

The `testing` package has built in support for generating CPU, memory, and block profiles.

- `-cpuprofile=$FILE` writes a CPU profile to `$FILE`.
- `-memprofile=$FILE`, writes a memory profile to `$FILE`, `-memprofilerate=N` adjusts the profile rate to `1/N`.
- `-blockprofile=$FILE`, writes a block profile to `$FILE`.

Using any of these flags also preserves the binary.

[source]
% go test -run=XXX -bench=. -cpuprofile=c.p bytes
% go tool pprof c.p

== Discussion

Are there any questions?

Perhaps it is time for a break.