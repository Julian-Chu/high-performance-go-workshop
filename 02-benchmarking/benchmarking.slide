Writing Go benchmarks
Mercari
11 Apr 2018

Dave Cheney
dave@cheney.net
http://dave.cheney.net/
@davecheney

* Benchmarking

In the prevoius section we looked at profiling _whole_ programs which is useful for answering high level questions like:

"Why is this program taking so long to run"

But sometimes you know where the problem is, and you want to isolate it so you can try to improve it.

This section focuses on how to construct useful benchmarks using the Go testing framework, and gives practical tips for avoiding the pitfalls.

* Benchmarking ground rules

Before you benchmark, you must have a stable environment to get repeatable results.

- The machine must be idle—don't profile on shared hardware, don't browse the web while waiting for a long benchmark to run.
- Watch out for power saving and thermal scaling.
- Avoid virtual machines and shared cloud hosting; they are too noisy for consistent measurements.

If you can afford it, buy dedicated performance test hardware. Rack it, disable all the power management and thermal scaling and never update the software on those machines.

For everyone else, have a before and after sample and run them multiple times to get consistent results.

* Using the testing package for benchmarking

The `testing` package has built in support for writing benchmarks.

.code examples/fib/fib_test.go /STARTFIB OMIT/,/ENDFIB OMIT/
.caption fib.go

.code examples/fib/fib_test.go /STARTBENCH OMIT/,/ENDBENCH OMIT/
.caption fib_test.go

DEMO: `go`test`-bench=.`./examples/fib`

* How benchmarks work

Each benchmark is run `b.N` times until `b.N` iterations takes longer than 1 second.

`b.N` starts at 1, if the benchmark completes in under 1 second `b.N` is increased and the benchmark run again.

`b.N` increases in the approximate sequence; 1, 2, 3, 5, 10, 20, 30, 50, 100, ...

 % go test -bench=. ./examples/fib
 BenchmarkFib-4             30000             46408 ns/op
 PASS
 ok      _/Users/dfc/devel/high-performance-go-workshop/examples/fib     1.910s

_Beware:_ below the μs mark you will start to see the relativistic effects of instruction reordering and code alignment. 

* Improving benchmark accuracy

You should aim for your benchmarks to run in the 10,000's of iterations.

If your benchmark runs for 100's or 10's of iterations, the average of those runs may not be stable. Run benchmarks longer to get more accuracy; `go`test`-benchtime=10s`

Also be aware that if you have a benchmark that runs in 10's of ns/op your benchmark may be less stable because of the effect of thermal scaling, memory locality, background processing, gc activity, etc.

Small values are hard to measure accurately. To address this run benchmarks multiple times; `go`test`-count=10`

_Tip:_ If this is required, codify it in a `Makefile` so everyone is comparing apples to apples.

* Benchstat

I'm going to introduce a tool by Russ Cox called [[https://godoc.org/golang.org/x/perf/cmd/benchstat][benchstat]].

Benchstat can take a set of benchmark runs and tell you how stable they are:

 % go get golang.org/x/perf/cmd/benchstat

We can use it to see how stable our benchmark setup is

 % cd examples/fib
 % % go test -bench=. -count=10 | tee old.txt
 % benchstat old.txt
 name     time/op
 Fib20-4  49.5µs ± 1%

* Comparing benchmarks

Determining the performance delta between two sets of benchmarks can be tedious and error prone. Benchstat can help us with this;

 % go test -c
 % mv fib.test fib.golden 

_Tip:_ copy your _before_ test binary to a new file so you have a known reference program.

DEMO: Improve `Fib`

 % go test -c
 % ./fib.golden -test.bench=. -test.count=10 > old.txt
 % ./fib.test -test.bench=. -test.count=10 > new.txt
 % benchstat old.txt new.txt
 name     old time/op  new time/op  delta
 Fib20-4  49.5µs ± 1%  32.0µs ± 1%  -35.38%  (p=0.000 n=10+10)

DEMO: `benchstat`{old,new}.txt`

* Comparing benchmarks (cont.)

There are three things to check when comparing benchmarks

- The variance ± in the old and new times. 1-2% is good, 3-5% is ok, greater than 5% and some of your samples will be considered unreliable. Be careful when comparing benchmarks where one side has a high variance, you may not be seeing an improvement.
- p value. p values lower than 0.05 are good, greater than 0.05 means the benchmark may not be statistically significant.
- Missing samples. benchstat will report how many of the old and new samples it considered to be valid, sometimes you may find only, say, 9 reported, even though you did `-count=10`, a 10% of lower rejection rate is ok, higher than 10% may indicate your setup is unstable and you may be comparing too few samples.

* Avoid benchmarking start up costs

Sometimes your benchmark has a once per run setup cost. `b.ResetTimer()` will can be used to ignore the time accrued in setup.

.code examples/reset.go /START1 OMIT/,/END1 OMIT/

If you have some expensive setup logic _per_loop_iteration, use `b.StopTimer()` and `b.StartTimer()` to pause the benchmark timer.

.code examples/reset.go /START2 OMIT/,/END2 OMIT/

* Benchmarking allocations

Allocation count and size is strongly correlated with benchmark time.

You can tell the `testing` framework to record the number of allocations made by code under test.
 
.code examples/benchmark.go

DEMO: `go`test`-run=^$`-bench=.`bufio`

_Note:_ you can also use the `go`test`-benchmem` flag to do the same for _all_ benchmarks.

DEMO: `go`test`-run=^$`-bench=.`-benchmem`bufio`

* Watch out for compiler optimisations

This example comes from [[https://github.com/golang/go/issues/14813#issue-140603392][issue 14813]]. How fast will this function benchmark?

.code examples/popcnt/popcnt_test.go /START OMIT/,/END OMIT/

* What happened?

 % go test -bench=. ./examples/popcnt

`popcnt` is a leaf function, so the compiler can inline it.

Because the function is inlined, the compiler can see it has no side effects, so the call is eliminated. This is what the compiler sees:

.code examples/popcnt/popcnt2_test.go /START OMIT/,/END OMIT/

The same optimisations that make real code fast, by removing unnecessary computation, are the same ones that remove benchmarks that have no observable side effects.

This is only going to get more common as the Go compiler improves.

DEMO: show how to fix popcnt

* Benchmark mistakes

The `for` loop is crucial to the operation of the benchmark.

Here are two incorrect benchmarks, can you explain what is wrong with them?

.code examples/benchfib/wrong_test.go /START OMIT/,/END OMIT/

* Profiling benchmarks

The `testing` package has built in support for generating CPU, memory, and block profiles.

- `-cpuprofile=$FILE` writes a CPU profile to `$FILE`.
- `-memprofile=$FILE`, writes a memory profile to `$FILE`, `-memprofilerate=N` adjusts the profile rate to `1/N`.
- `-blockprofile=$FILE`, writes a block profile to `$FILE`.

Using any of these flags also preserves the binary.

    % go test -run=XXX -bench=. -cpuprofile=c.p bytes
    % go tool pprof bytes.test c.p

_Note:_ use `-run=XXX` to disable tests, you only want to profile benchmarks. You can also use `-run=^$` to accomplish the same thing.

* Discussion

Are there any questions?

Perhaps it is time for a break.

* Next

[[../03-compiler-optimisations/compiler-optimisations.slide][Compiler optimisations]]
